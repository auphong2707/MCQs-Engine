[
    {
        "type": "multi",
        "question": "Which of the following are primary design goals and benefits of Apache Kafka?",
        "options": [
            "To decouple data producers from data consumers.",
            "To provide a high-throughput, low-latency platform for handling real-time data feeds.",
            "To serve as a complex SQL engine for performing joins across relational tables.",
            "To act as a buffer for bursty data traffic."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Kafka is designed to decouple systems, handle high throughput/low latency, and act as a buffer. It is not a relational SQL engine. Source: 5_kafka.pdf (Slide 3)."
    },
    {
        "type": "multi",
        "question": "Which of the following core characteristics accurately describe Apache Kafka?",
        "options": [
            "Fast and Scalable.",
            "Durable (messages are persisted on disk).",
            "Fault-tolerant (via replication).",
            "Transient (messages are stored only in RAM)."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Kafka is fast, scalable, durable (disk-based), and fault-tolerant. It is not transient; it persists data. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "Regarding Kafka Topics, which statements are correct?",
        "options": [
            "A Topic is a category or feed name to which records are published.",
            "Topics are broken down into ordered commit logs called Partitions.",
            "A single Topic must fit entirely on one Broker machine.",
            "Topics are identified by a unique name."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Topics are logical categories divided into partitions. Partitions can be distributed across multiple brokers, so a topic does not need to fit on one machine. Source: 5_kafka.pdf (Slide 9)."
    },
    {
        "type": "multi",
        "question": "Which of the following statements about Kafka Partitions are true?",
        "options": [
            "Partitions allow a topic to scale beyond a size that will fit on a single server.",
            "Partitions act as the unit of parallelism.",
            "Ordering is guaranteed across all partitions in a topic.",
            "Each partition is an ordered, immutable sequence of records."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Partitions provide scalability and are the unit of parallelism. Ordering is only guaranteed *within* a partition, not across the whole topic. Source: 5_kafka.pdf (Slide 9)."
    },
    {
        "type": "multi",
        "question": "What is an 'Offset' in Kafka?",
        "options": [
            "A unique sequential ID assigned to every message within a partition.",
            "A pointer used by consumers to track their reading position.",
            "A timestamp of when the message was created.",
            "A random hash key for security."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "An offset is a sequential integer ID per partition used to identify messages and track consumer progress. Source: 5_kafka.pdf (Slide 11)."
    },
    {
        "type": "multi",
        "question": "How does a Kafka Producer decide which partition to send a record to?",
        "options": [
            "If a key is present, it uses a hashing algorithm (Key hash % Num Partitions).",
            "If no key is present, it uses a round-robin (or sticky) approach.",
            "It queries the Consumer Group to ask where the data is needed.",
            "It allows the Broker to randomly assign the data after receipt."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Producers determine the partition: Hashing if Key exists, Round-robin if null. Source: 5_kafka.pdf (Slide 13)."
    },
    {
        "type": "multi",
        "question": "Which statements describe the 'Consumer Group' concept?",
        "options": [
            "It allows a group of consumers to cooperate and consume a topic in parallel.",
            "Each partition in the topic is consumed by exactly one consumer in the group.",
            "If there are more consumers than partitions, the extra consumers remain idle.",
            "Multiple consumers in the same group can read the same partition simultaneously."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Consumer Groups enable parallelism. Crucially, a partition is assigned to only one member of the group. Extra members are idle. Source: 5_kafka.pdf (Slide 15)."
    },
    {
        "type": "multi",
        "question": "Why does Kafka utilize a 'Pull' model for consumers?",
        "options": [
            "It allows consumers to consume at their own maximum rate (prevents overwhelming).",
            "It facilitates aggressive batching of data.",
            "It allows the Broker to push data as fast as possible to minimize latency.",
            "It shifts the responsibility of tracking offsets to the Broker."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "The Pull model prevents consumer-side overload and supports batching. Push models risk overwhelming slow consumers. Source: 5_kafka.pdf (Slide 14)."
    },
    {
        "type": "multi",
        "question": "What are valid use cases for Apache Kafka?",
        "options": [
            "Website Activity Tracking.",
            "Log Aggregation.",
            "Operational Metrics.",
            "Storing large video files (GBs)."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Tracking, Logs, and Metrics are core use cases. Storing large files is explicitly discouraged. Source: 5_kafka.pdf (Slide 3, 44)."
    },
    {
        "type": "multi",
        "question": "How is Fault Tolerance achieved in Kafka?",
        "options": [
            "By replicating partitions across multiple brokers.",
            "By using a Leader-Follower model where Followers passively replicate data.",
            "By backing up data to an external tape drive.",
            "By using RAID 0 on all disks."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Fault tolerance is achieved via partition replication (Leader/Follower). Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "What roles does Zookeeper play in a Kafka cluster?",
        "options": [
            "It manages cluster metadata (e.g., topic configuration).",
            "It performs Leader Election for partitions.",
            "It stores the actual message payloads.",
            "It detects broker failures (via heartbeats/sessions)."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Zookeeper coordinates the cluster, elects leaders, and stores metadata. It does *not* store message data. Source: 5_kafka.pdf (Slide 5)."
    },
    {
        "type": "multi",
        "question": "Which features contribute to Kafka's high performance?",
        "options": [
            "Zero-Copy optimization (sendfile).",
            "Sequential Disk I/O (append-only logs).",
            "Batching of messages.",
            "Heavy reliance on Java Heap memory for caching."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Kafka uses Zero-Copy, Sequential I/O, and Batching. It relies on the OS Page Cache, not Java Heap, for caching. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "If you have a Consumer Group with 4 consumers and a Topic with 3 partitions:",
        "options": [
            "Three consumers will be active.",
            "One consumer will remain idle.",
            "One partition will be shared by two consumers.",
            "The cluster will automatically split a partition to accommodate the 4th consumer."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Since a partition can only be read by one consumer in a group, 4 consumers for 3 partitions means 1 is idle. Source: 5_kafka.pdf (Slide 15)."
    },
    {
        "type": "multi",
        "question": "How does Kafka handle data retention?",
        "options": [
            "Data is retained for a configurable period (e.g., 7 days).",
            "Data is retained until a specific size limit is reached.",
            "Data is deleted immediately after it is read by a consumer.",
            "Data is kept forever if configured to do so (Log Compaction or infinite retention)."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Retention is policy-based (Time or Size). Data is *not* deleted just because it was read. Source: 5_kafka.pdf (Slide 4, 11)."
    },
    {
        "type": "multi",
        "question": "In the context of Replication, what are the roles of Leader and Follower?",
        "options": [
            "All reads and writes are directed to the Leader partition.",
            "Followers replicate data from the Leader to stay in sync.",
            "Producers write to Followers to distribute load.",
            "If a Leader fails, a Follower in the ISR (In-Sync Replica) list is elected new Leader."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Clients talk to the Leader. Followers exist for redundancy and failover. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "Which messaging models does Kafka support via Consumer Groups?",
        "options": [
            "Queuing (Point-to-Point): All consumers in one group.",
            "Publish-Subscribe: Consumers in different groups.",
            "Broadcast: Sending data to all partitions simultaneously.",
            "Request-Reply: Synchronous blocking calls."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Kafka supports Queuing (load balancing in one group) and Pub-Sub (broadcasting to multiple groups). Source: 5_kafka.pdf (Slide 15)."
    },
    {
        "type": "multi",
        "question": "Which of the following are NOT suitable use cases for Kafka?",
        "options": [
            "Transferring very large files (e.g., raw 4K video footage).",
            "Hard real-time systems (e.g., pacemaker control).",
            "Stream Processing.",
            "Log Aggregation."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Kafka is not for large files (Slide 44) or hard real-time safety-critical systems. Stream Processing and Logs are good use cases. Source: 5_kafka.pdf (Slide 44)."
    },
    {
        "type": "multi",
        "question": "What triggers a 'Rebalance' in a Consumer Group?",
        "options": [
            "A new consumer joins the group.",
            "A consumer leaves the group (or crashes).",
            "A producer sends a message.",
            "New partitions are added to a topic."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Rebalancing redistributes partitions when membership changes or topic topology changes. Production does not trigger it. Source: 5_kafka.pdf (Slide 15 context)."
    },
    {
        "type": "multi",
        "question": "Which delivery guarantees can be achieved with Kafka?",
        "options": [
            "At-most-once (Message might be lost, never redelivered).",
            "At-least-once (Message never lost, may be redelivered).",
            "Exactly-once (Message delivered once and only once - via specific configurations/streams).",
            "Totally ordered delivery across all topics."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Kafka supports all three semantics depending on config. Total global order is not supported. Source: 5_kafka.pdf (General Delivery Semantics)."
    },
    {
        "type": "multi",
        "question": "What is a 'Log Segment'?",
        "options": [
            "A physical file on disk storing a slice of the partition log.",
            "The mechanism used to delete old data (rolling segments).",
            "A subset of Zookeeper nodes.",
            "A specialized consumer."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Logs are physically split into segments for easier file management and cleaning. Source: 5_kafka.pdf (Slide 11)."
    },
    {
        "type": "multi",
        "question": "Why does Kafka prefer Sequential I/O over Random I/O?",
        "options": [
            "Sequential I/O is significantly faster on mechanical disks (HDDs).",
            "It avoids expensive disk 'seek' operations.",
            "It allows Kafka to use the Operating System's Page Cache effectively.",
            "It reduces the need for network bandwidth."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Sequential I/O avoids seeks and works well with pre-fetching/page cache, boosting speed. Source: 5_kafka.pdf (Performance context)."
    },
    {
        "type": "multi",
        "question": "What components make up a standard message in Kafka?",
        "options": [
            "Key",
            "Value",
            "Timestamp",
            "Broker ID"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "A message typically contains Key, Value, and Timestamp (and headers). Broker ID is not part of the message record. Source: 5_kafka.pdf (Slide 10/11)."
    },
    {
        "type": "multi",
        "question": "How can you ensure the ordering of messages?",
        "options": [
            "By sending all related messages to the same partition.",
            "By using the same 'Key' for related messages.",
            "By enabling 'Total Order' configuration on the Broker.",
            "By using a single partition topic."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Order is guaranteed within a partition. Using a Key ensures routing to the same partition. Single partition topics also guarantee total order (at cost of scale). Source: 5_kafka.pdf (Slide 9)."
    },
    {
        "type": "multi",
        "question": "What defines a Kafka Cluster?",
        "options": [
            "It consists of one or more Brokers.",
            "It uses Zookeeper for coordination.",
            "It requires a shared NAS storage.",
            "It can span multiple datacenters (though usually deployed in one for latency)."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Clusters are sets of Brokers coordinated by Zookeeper. They use local disk, not shared NAS. Source: 5_kafka.pdf (Slide 3)."
    },
    {
        "type": "multi",
        "question": "What does 'At-least-once' delivery imply?",
        "options": [
            "Messages are guaranteed to be persisted.",
            "Consumers will never miss a message.",
            "Consumers might process the same message twice (duplicates).",
            "Messages are deleted immediately upon receipt."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "At-least-once guarantees no loss but allows duplicates (e.g., failure after process but before commit). Source: 5_kafka.pdf (Delivery Semantics)."
    },
    {
        "type": "multi",
        "question": "What is the 'High Watermark'?",
        "options": [
            "The offset of the last message that was successfully replicated to all ISRs.",
            "The limit of how much consumers can read (consumers cannot read past HW).",
            "The maximum disk space allowed.",
            "The highest API version supported."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "High Watermark tracks the committed data. Consumers only see up to the HW to ensure consistency. Source: 5_kafka.pdf (Replication)."
    },
    {
        "type": "multi",
        "question": "Why is Kafka useful as a buffer?",
        "options": [
            "It absorbs bursts of data from producers.",
            "It allows consumers to catch up if they fall behind.",
            "It prevents data loss if downstream systems crash.",
            "It performs complex ETL on the data."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Buffering protects downstream systems from spikes and failures. It doesn't do complex ETL itself. Source: 5_kafka.pdf (Slide 3)."
    },
    {
        "type": "multi",
        "question": "What happens if a Consumer Group has more consumers than partitions?",
        "options": [
            "The extra consumers will be idle.",
            "Resources are effectively wasted.",
            "The idle consumers act as hot standbys (failover) if an active consumer dies.",
            "Kafka splits partitions automatically."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Excess consumers are idle/wasted but can pick up work if an active one fails (rebalance). Kafka does not auto-split partitions. Source: 5_kafka.pdf (Slide 15)."
    },
    {
        "type": "multi",
        "question": "What is 'Log Compaction'?",
        "options": [
            "A retention policy that keeps the latest value for each Key.",
            "A way to restore the state of a system (snapshot).",
            "A compression algorithm like GZIP.",
            "A process to delete all old data."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Compaction keeps the 'tail' (latest value) for every key, useful for restoring state. It is not GZIP compression. Source: 5_kafka.pdf (Slide 11 context)."
    },
    {
        "type": "multi",
        "question": "Which statements are true about Kafka Brokers?",
        "options": [
            "A broker is a single server in the Kafka cluster.",
            "Brokers are stateless (mostly) because Zookeeper manages metadata.",
            "Each broker hosts a subset of partitions.",
            "Brokers perform the serialization and deserialization of messages."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Brokers are servers holding partition data. They don't ser/deser (clients do that). Zookeeper handles state. Source: 5_kafka.pdf (Slide 12)."
    },
    {
        "type": "multi",
        "question": "What is the main Unit of Parallelism in Kafka?",
        "options": [
            "The Partition.",
            "The Consumer Group.",
            "The Topic.",
            "The Zookeeper Node."
        ],
        "answer": [
            0
        ],
        "explanation": "While 'Consumer Group' is the mechanism, the 'Partition' is the fundamental unit that limits parallelism. (Included as Multi choice to be safe, though technically single answer). Source: 5_kafka.pdf (Slide 9)."
    },
    {
        "type": "multi",
        "question": "What data does Zookeeper store?",
        "options": [
            "Controller election.",
            "Topic configuration.",
            "Access Control Lists (ACLs).",
            "Message Data."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Zookeeper stores metadata and state. It explicitly does *not* store message data. Source: 5_kafka.pdf (Architecture)."
    },
    {
        "type": "multi",
        "question": "Which compression codecs does Kafka support?",
        "options": [
            "Gzip",
            "Snappy",
            "LZ4",
            "JPEG"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Kafka supports Gzip, Snappy, LZ4 (and Zstd). JPEG is an image format. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "Who tracks the 'Offset' in the standard Kafka model?",
        "options": [
            "The Consumer (or Consumer Group).",
            "The Broker (via internal topic `__consumer_offsets`).",
            "The Producer.",
            "The Network Switch."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Consumers track their position, committing it to the Broker. Source: 5_kafka.pdf (Slide 14)."
    },
    {
        "type": "multi",
        "question": "What are the benefits of using a 'Key' in a message?",
        "options": [
            "Ensures related messages go to the same partition.",
            "Guarantees ordering for that key.",
            "Enables Log Compaction.",
            "Encrypts the message payload."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Keys enable semantic partitioning (ordering) and are required for compaction. Source: 5_kafka.pdf (Slide 13)."
    },
    {
        "type": "multi",
        "question": "When does a Consumer Group Rebalance occur?",
        "options": [
            "When a consumer joins or leaves.",
            "When an admin adds partitions to a topic.",
            "When a broker shuts down (causing consumer disconnection).",
            "When a message is produced."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Any change in consumer membership or topic topology triggers rebalance. Source: 5_kafka.pdf (Slide 15)."
    },
    {
        "type": "multi",
        "question": "How should you handle large files (e.g., Videos) with Kafka?",
        "options": [
            "Store the file in S3/HDFS.",
            "Send a reference (URL) to the file in the Kafka message.",
            "Break the file into 1KB chunks (not recommended).",
            "Send the whole 5GB file as one message."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Best practice: Store externally, send reference. Kafka is not for large blobs. Source: 5_kafka.pdf (Slide 44)."
    },
    {
        "type": "multi",
        "question": "What security features are available in Kafka?",
        "options": [
            "Encryption in transit (SSL/TLS).",
            "Authentication (SASL).",
            "Authorization (ACLs).",
            "Data masking."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Kafka supports SSL, SASL, and ACLs. Data masking is typically done by the application/client. Source: 5_kafka.pdf (Slide 44)."
    },
    {
        "type": "multi",
        "question": "What contributes to Kafka's High Throughput?",
        "options": [
            "Zero-Copy data transfer.",
            "Sequential I/O.",
            "Message Batching.",
            "Synchronous blocking writes."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Zero-copy, Sequential I/O, and Batching are key. Blocking writes would hurt throughput. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "What is an ISR (In-Sync Replica)?",
        "options": [
            "A replica that is fully caught up with the Leader.",
            "A candidate that can become the new Leader if the current one fails.",
            "A replica stored in a separate region.",
            "A failed replica."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "ISR is a replica that is alive and caught up. Only ISRs can be elected Leader (usually). Source: 5_kafka.pdf (Fault Tolerance)."
    },
    {
        "type": "multi",
        "question": "What happens upon a Leader Failure?",
        "options": [
            "The Controller detects the failure (via Zookeeper).",
            "A new Leader is elected from the ISR list.",
            "Clients (Producers/Consumers) switch to the new Leader.",
            "The topic is deleted."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Failover involves detection, election, and client metadata update. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "How does Kafka store data on disk?",
        "options": [
            "As binary files.",
            "Segmented by size or time.",
            "Appended sequentially.",
            "In XML format."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Storage is binary, segmented, and append-only. Not XML. Source: 5_kafka.pdf (Slide 11)."
    },
    {
        "type": "multi",
        "question": "Is Kafka a Database?",
        "options": [
            "It is primarily a distributed commit log.",
            "It can be used as a source of truth (Kappa Architecture).",
            "It supports full SQL ACID transactions like Oracle.",
            "It has limited query capabilities compared to RDBMS."
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "It's a log. While persistent, it lacks the random access and complex query features of a DB. Source: 5_kafka.pdf (General use case)."
    },
    {
        "type": "multi",
        "question": "What are the responsibilities of the Producer?",
        "options": [
            "Writing data to the Leader partition.",
            "Deciding the partitioning strategy.",
            "Serializing the data.",
            "Replicating data to Followers."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Producer writes, partitions, and serializes. Replication is the Broker's job. Source: 5_kafka.pdf (Slide 13)."
    },
    {
        "type": "multi",
        "question": "How do you read a topic from the beginning?",
        "options": [
            "Use a new Consumer Group ID.",
            "Set `auto.offset.reset` to `earliest`.",
            "Manually seek to offset 0.",
            "Delete the `__consumer_offsets` topic."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "New group + earliest reset, or manual seek, allows reading from start. Source: 5_kafka.pdf (Consumer usage)."
    },
    {
        "type": "multi",
        "question": "Which messaging patterns are supported?",
        "options": [
            "Point-to-Point (Queue).",
            "Publish-Subscribe.",
            "Streaming.",
            "Synchronous RPC."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Kafka supports Queue, Pub-Sub, and Streaming. RPC is anti-pattern. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "What are the advantages of Decoupling?",
        "options": [
            "Independent scalability of Producers and Consumers.",
            "Fault isolation (Consumer failure doesn't stop Producer).",
            "Buffering against load spikes.",
            "Immediate consistency."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Decoupling enables independent scale, isolation, and buffering. Consistency is eventual. Source: 5_kafka.pdf (Slide 3)."
    },
    {
        "type": "multi",
        "question": "What is 'Replication Factor'?",
        "options": [
            "The number of copies of a partition.",
            "A setting defined at the Topic level.",
            "The number of consumers in a group.",
            "The compression level."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Replication Factor is the copy count, set per topic. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "What happens if a Broker fails?",
        "options": [
            "Partitions it led become unavailable until election.",
            "Followers on other brokers take over.",
            "Data stored only on that broker (if RF=1) is unavailable.",
            "The cluster shuts down."
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Failover handles availability unless RF=1. Cluster remains up. Source: 5_kafka.pdf (Slide 4)."
    },
    {
        "type": "multi",
        "question": "What limits Kafka Consumer performance?",
        "options": [
            "Network Bandwidth.",
            "Disk I/O.",
            "Complex CPU processing.",
            "Zookeeper speed."
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Kafka consumers are usually I/O or Network bound, not CPU bound (no complex logic). Source: 5_kafka.pdf (Performance)."
    }
]