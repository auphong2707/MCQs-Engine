[
    {
        "type": "multi",
        "question": "Which of the following code snippets correctly initializes a SparkSession in a standard PySpark application?",
        "options": [
            "spark = SparkSession.builder.getOrCreate()",
            "spark = SparkSession.new()",
            "spark = SparkContext.getOrCreate()",
            "spark = SparkSession(sc)  # Assuming sc is a valid SparkContext"
        ],
        "answer": [
            0,
            3
        ],
        "explanation": "Option 0 is the standard builder pattern for creating a session. Option 3 is a valid legacy way to create a session if a SparkContext (`sc`) already exists. Option 1 is invalid syntax. Option 2 creates a Context, not a Session."
    },
    {
        "type": "multi",
        "question": "You need to read a JSON file where the schema should be inferred. Which commands are valid?",
        "options": [
            "spark.read.format(\"json\").load(\"data.json\")",
            "spark.read.json(\"data.json\")",
            "spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(\"data.json\")",
            "spark.load(\"data.json\", format=\"json\")"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Options 0 and 1 are the standard methods to read JSON. Option 2 is also valid, though `inferSchema` is often `true` by default or implied for JSON depending on configuration, explicitly setting it is valid API usage. Option 4 is incorrect syntax; `load` is a method of `DataFrameReader` (accessed via `spark.read`), not `spark` directly."
    },
    {
        "type": "multi",
        "question": "Which of the following snippets correctly defines a programmatic schema using `StructType`?",
        "options": [
            "StructType([StructField(\"col1\", StringType(), True)])",
            "Schema([Field(\"col1\", StringType(), True)])",
            "StructType([Column(\"col1\", \"string\")])",
            "StructType().add(\"col1\", StringType(), True)"
        ],
        "answer": [
            0,
            3
        ],
        "explanation": "Option 0 is the standard list-based construction. Option 3 is a valid chained method to add fields to a StructType. Options 1 and 2 use non-existent classes (`Schema`, `Field`, `Column` in this context)."
    },
    {
        "type": "multi",
        "question": "How can you select the column \"price\" and add 5 to it in a DataFrame `df`?",
        "options": [
            "df.select(col(\"price\") + 5)",
            "df.selectExpr(\"price + 5\")",
            "df.select(\"price\" + 5)",
            "df.select(df[\"price\"] + 5)"
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Option 0 uses the column object `col()`. Option 1 uses a SQL expression string. Option 3 uses the DataFrame bracket notation. Option 2 is incorrect because adding an integer to a string column name directly in Python throws a type error; it must be a Column object."
    },
    {
        "type": "multi",
        "question": "Which method is used to remove a specific column from a DataFrame?",
        "options": [
            "df.drop(\"colName\")",
            "df.remove(\"colName\")",
            "df.select(\"*\", except(\"colName\"))",
            "df.delete(\"colName\")"
        ],
        "answer": [
            0
        ],
        "explanation": "Only `drop` is the correct method for removing columns in PySpark DataFrames. `remove` and `delete` do not exist. There is no `except` function inside `select` used this way."
    },
    {
        "type": "multi",
        "question": "Which of the following are valid ways to filter a DataFrame `df` for rows where \"age\" > 21?",
        "options": [
            "df.filter(col(\"age\") > 21)",
            "df.where(\"age > 21\")",
            "df.filter(\"age > 21\")",
            "df.select(\"age\" > 21)"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Option 0 uses the Column expression. Option 1 uses `where` with a SQL string. Option 2 uses `filter` with a SQL string (synonym for `where`). Option 3 returns a boolean column result, it does not filter the rows."
    },
    {
        "type": "multi",
        "question": "You want to sort a DataFrame by \"count\" in descending order. Which snippets work?",
        "options": [
            "df.sort(col(\"count\").desc())",
            "df.orderBy(desc(\"count\"))",
            "df.sort(\"count\", ascending=False)",
            "df.orderBy(\"count\", desc=True)"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Option 0 and 1 are standard sorting methods using Column expressions or functions. Option 2 is valid syntax for `sort`. Option 3 is incorrect because `orderBy` does not accept `desc` as a keyword argument in that format."
    },
    {
        "type": "multi",
        "question": "Which command correctly renames a column from \"oldName\" to \"newName\"?",
        "options": [
            "df.withColumnRenamed(\"oldName\", \"newName\")",
            "df.rename(\"oldName\", \"newName\")",
            "df.select(col(\"oldName\").alias(\"newName\"), \"*\")",
            "df.withColumn(\"newName\", col(\"oldName\"))"
        ],
        "answer": [
            0
        ],
        "explanation": "Option 0 is the dedicated method for renaming. Option 1 is not a PySpark method (it's Pandas). Option 2 and 4 create a new column (or a projection) but Option 2 requires careful handling of other columns (it doesn't just rename in place easily without excluding the old one), and Option 4 creates a duplicate, it doesn't rename/remove the old one."
    },
    {
        "type": "multi",
        "question": "Which of the following aggregations can be performed using `df.groupBy(\"col\").agg(...)`?",
        "options": [
            "count(\"id\")",
            "sum(\"salary\")",
            "collect_list(\"tags\")",
            "map_values(\"items\")"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Options 0, 1, and 2 are standard aggregation functions. Option 3 `map_values` is a collection function, not an aggregate function used in grouping contexts."
    },
    {
        "type": "multi",
        "question": "Which joins are supported by the `df.join()` method?",
        "options": [
            "inner",
            "left_outer",
            "left_semi",
            "outer_cross"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "`inner`, `left_outer`, and `left_semi` are valid join types. There is no join type named `outer_cross`; cross joins are usually handled via `.crossJoin()` or explicitly specifying `cross`."
    },
    {
        "type": "multi",
        "question": "You have a DataFrame `df`. How do you display the schema tree to the console?",
        "options": [
            "df.printSchema()",
            "df.schema()",
            "print(df.schema)",
            "df.showSchema()"
        ],
        "answer": [
            0
        ],
        "explanation": "Option 0 is the correct method to print the formatted schema tree. Option 1 is incorrect (schema is a property, not a method). Option 2 prints the `StructType` object representation, not the formatted tree. Option 3 does not exist."
    },
    {
        "type": "multi",
        "question": "Which of the following snippets correctly writes a DataFrame to a Parquet file?",
        "options": [
            "df.write.format(\"parquet\").save(\"path/to/output\")",
            "df.write.parquet(\"path/to/output\")",
            "df.save(\"path/to/output\", format=\"parquet\")",
            "df.write.mode(\"overwrite\").parquet(\"path/to/output\")"
        ],
        "answer": [
            0,
            1,
            3
        ],
        "explanation": "Options 0, 1, and 3 are valid writer commands. Option 2 is incorrect because `save` is not a direct method of DataFrame, it belongs to `DataFrameWriter` (`df.write`)."
    },
    {
        "type": "multi",
        "question": "Which of the following window specifications partitions by 'dept' and orders by 'salary' descending?",
        "options": [
            "Window.partitionBy(\"dept\").orderBy(desc(\"salary\"))",
            "Window.groupBy(\"dept\").sort(\"salary\", ascending=False)",
            "Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())",
            "Window.map(\"dept\").order(\"salary\")"
        ],
        "answer": [
            0,
            2
        ],
        "explanation": "Option 0 and 2 are correct usages of the Window API. Option 1 is incorrect (`groupBy` is not part of Window Spec, `sort` is not used there). Option 3 uses invalid method names."
    },
    {
        "type": "multi",
        "question": "How do you perform a cross join between `df1` and `df2`?",
        "options": [
            "df1.crossJoin(df2)",
            "df1.join(df2, how=\"cross\")",
            "df1.join(df2)",
            "df1.merge(df2, how=\"cross\")"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Option 0 is the explicit method for cross joins. Option 1 is valid if the join condition is omitted or valid for cross. Option 2 defaults to inner join and may require a condition. Option 4 is pandas syntax."
    },
    {
        "type": "multi",
        "question": "Which snippet calculates the rank of rows within a window?",
        "options": [
            "rank().over(windowSpec)",
            "df.rank(windowSpec)",
            "dense_rank().over(windowSpec)",
            "windowSpec.rank()"
        ],
        "answer": [
            0,
            2
        ],
        "explanation": "Option 0 and 2 use the correct syntax: an analytic function followed by `.over(windowSpec)`. `rank` and `dense_rank` are both valid ranking functions. The other options are invalid syntax."
    },
    {
        "type": "multi",
        "question": "In Structured Streaming, how do you read from a directory of JSON files as a stream?",
        "options": [
            "spark.readStream.format(\"json\").load(\"path/to/dir\")",
            "spark.readStream.json(\"path/to/dir\")",
            "spark.read.stream(\"json\", \"path/to/dir\")",
            "spark.createStream(\"path/to/dir\", \"json\")"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Options 0 and 1 are the correct API calls for the `DataStreamReader`. Options 2 and 3 do not exist."
    },
    {
        "type": "multi",
        "question": "Which output modes are valid in Structured Streaming?",
        "options": [
            "complete",
            "append",
            "update",
            "overwrite"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "`complete`, `append`, and `update` are the three supported output modes in Structured Streaming. `overwrite` is a save mode for batch jobs, not a streaming output mode."
    },
    {
        "type": "multi",
        "question": "Which function is used to handle late data in Structured Streaming by defining a threshold?",
        "options": [
            "withWatermark()",
            "withThreshold()",
            "handleLateData()",
            "watermark()"
        ],
        "answer": [
            0
        ],
        "explanation": "`withWatermark()` is the DataFrame method used to define the event time column and the threshold for late data."
    },
    {
        "type": "multi",
        "question": "Which code snippet creates a 10-minute tumbling window on the 'timestamp' column?",
        "options": [
            "window(col(\"timestamp\"), \"10 minutes\")",
            "window(col(\"timestamp\"), \"10 minutes\", \"10 minutes\")",
            "tumbling_window(\"timestamp\", \"10 minutes\")",
            "window(\"timestamp\", \"10m\")"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Option 0 creates a tumbling window (window size equals slide size implicitly). Option 1 explicitly sets the slide size equal to window size, also creating a tumbling window. Option 2 is not a valid function name. Option 3 is valid if \"timestamp\" is a string column name, but `window` generally expects a Column object or name, and \"10 minutes\" is the preferred string format (though \"10m\" might work in some contexts, the standard full format is safer)."
    },
    {
        "type": "multi",
        "question": "How do you start a streaming query that writes to memory?",
        "options": [
            "df.writeStream.format(\"memory\").queryName(\"myQuery\").start()",
            "df.writeStream.outputMode(\"complete\").format(\"memory\").queryName(\"myQuery\").start()",
            "df.saveStream(\"memory\")",
            "df.writeStream.start(format=\"memory\")"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Options 0 and 1 are correct. Note that for the memory sink, a `queryName` is usually required to query the table later. Option 2 is invalid syntax. Option 4 is incorrect; `start` generally takes the path or is empty, configuration happens before `start`."
    },
    {
        "type": "multi",
        "question": "Which of the following are valid trigger types you can set in a streaming query?",
        "options": [
            "processingTime",
            "once",
            "continuous",
            "batch"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "`processingTime`, `once`, and `continuous` are valid trigger configurations. `batch` is not a specific trigger keyword (though `once` effectively runs a batch)."
    },
    {
        "type": "multi",
        "question": "You want to create a new DataFrame `df2` that contains all rows from `df1` where the column 'color' is not null. Which code works?",
        "options": [
            "df1.filter(col(\"color\").isNotNull())",
            "df1.where(\"color IS NOT NULL\")",
            "df1.dropna(subset=[\"color\"])",
            "df1.select(\"color\").nonNull()"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Option 0 uses the column method `isNotNull()`. Option 1 uses SQL syntax. Option 2 uses the DataFrame method `dropna` restricted to the specific column. Option 3 is invalid syntax."
    },
    {
        "type": "multi",
        "question": "Which functions can be used to convert a timestamp string to a date type?",
        "options": [
            "to_date()",
            "to_timestamp()",
            "date_format()",
            "cast(\"date\")"
        ],
        "answer": [
            0
        ],
        "explanation": "`to_date()` is the specific function for casting to a date type. `to_timestamp` creates a timestamp (date + time). `date_format` returns a string. `cast` is a method on a Column, not a standalone function in this context (e.g., `col(\"x\").cast(\"date\")` would be valid, but just `cast(\"date\")` is not)."
    },
    {
        "type": "multi",
        "question": "How can you register a DataFrame `df` as a temporary view named \"sales\"?",
        "options": [
            "df.createOrReplaceTempView(\"sales\")",
            "df.registerTempTable(\"sales\")",
            "df.createTempView(\"sales\")",
            "spark.catalog.createView(\"sales\", df)"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Option 0 is the standard modern method. Option 1 is the older (deprecated but working) alias. Option 2 is also valid but will fail if the view exists. Option 4 is not standard syntax."
    },
    {
        "type": "multi",
        "question": "Which of the following correctly calculates the average salary per department?",
        "options": [
            "df.groupBy(\"dept\").avg(\"salary\")",
            "df.groupBy(\"dept\").agg(avg(\"salary\"))",
            "df.groupBy(\"dept\").mean(\"salary\")",
            "df.aggregate(\"dept\", avg(\"salary\"))"
        ],
        "answer": [
            0,
            1,
            2
        ],
        "explanation": "Option 0 is the shorthand. Option 1 is the explicit aggregation method. Option 2 is an alias for `avg`. Option 3 is not a valid method name."
    },
    {
        "type": "multi",
        "question": "What happens when you run `df.explain()`?",
        "options": [
            "It prints the physical and logical plans to the console.",
            "It returns the schema of the DataFrame.",
            "It triggers the job execution.",
            "It saves the execution graph to a file."
        ],
        "answer": [
            0
        ],
        "explanation": "`explain()` is used to debug and understand the query execution plan (logical and physical) generated by Catalyst."
    },
    {
        "type": "multi",
        "question": "Which method allows you to use SQL queries directly on registered tables?",
        "options": [
            "spark.sql(\"SELECT * FROM table\")",
            "df.sql(\"SELECT * FROM table\")",
            "spark.query(\"SELECT * FROM table\")",
            "sc.sql(\"SELECT * FROM table\")"
        ],
        "answer": [
            0
        ],
        "explanation": "`spark.sql()` is the entry point for executing SQL queries. `df` objects do not have a `sql` method. `sc` is the SparkContext and does not execute SQL directly."
    },
    {
        "type": "multi",
        "question": "Which column function creates an array from multiple columns?",
        "options": [
            "array()",
            "collect_list()",
            "struct()",
            "create_array()"
        ],
        "answer": [
            0
        ],
        "explanation": "`array()` creates an array column from a list of columns in the same row. `collect_list` is an aggregate function (across rows). `struct` creates a StructType, not an array."
    },
    {
        "type": "multi",
        "question": "You need to fill null values in column 'A' with 0 and column 'B' with 1. Which code works?",
        "options": [
            "df.fillna({'A': 0, 'B': 1})",
            "df.na.fill({'A': 0, 'B': 1})",
            "df.fillna([0, 1], subset=['A', 'B'])",
            "df.replace(None, {'A': 0, 'B': 1})"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Options 0 and 1 correctly use a dictionary to specify different fill values for different columns. Option 2 is invalid (cannot pass a list of values like that). Option 3 is invalid syntax for `replace`."
    },
    {
        "type": "multi",
        "question": "Which of the following code snippets correctly saves a DataFrame to a JDBC source?",
        "options": [
            "df.write.jdbc(url=myUrl, table=\"table\", properties=myProps)",
            "df.write.format(\"jdbc\").option(\"url\", myUrl).option(\"dbtable\", \"table\").save()",
            "df.toJDBC(myUrl, \"table\")",
            "spark.write.jdbc(df, myUrl, \"table\")"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Option 0 uses the direct `jdbc` method on DataFrameWriter. Option 1 uses the generic `format` and `option` syntax. The others are invalid."
    },
    {
        "type": "multi",
        "question": "In Structured Streaming, what does `spark.readStream.schema(mySchema)` do?",
        "options": [
            "It enforces the specified schema on the input stream.",
            "It infers the schema from the first file.",
            "It is optional for CSV and JSON files.",
            "It converts the stream to a static DataFrame."
        ],
        "answer": [
            0
        ],
        "explanation": "In Structured Streaming, you generally *must* specify the schema for file-based sources (like JSON/CSV) to avoid the cost of inference or errors. It enforces this schema."
    },
    {
        "type": "multi",
        "question": "Which function allows you to pivot a column's distinct values into new columns?",
        "options": [
            "df.groupBy(\"col1\").pivot(\"col2\").sum(\"col3\")",
            "df.pivot(\"col2\").groupBy(\"col1\").sum(\"col3\")",
            "df.crosstab(\"col1\", \"col2\")",
            "df.groupBy(\"col1\").rotate(\"col2\")"
        ],
        "answer": [
            0
        ],
        "explanation": "`pivot` is a method of `GroupedData`, so it must follow a `groupBy`. Option 2 `crosstab` calculates counts, not general aggregations like sum, and returns a DataFrame, not a GroupedData object for further aggregation."
    },
    {
        "type": "multi",
        "question": "How do you drop duplicate rows based on specific columns 'A' and 'B'?",
        "options": [
            "df.dropDuplicates(['A', 'B'])",
            "df.drop_duplicates(subset=['A', 'B'])",
            "df.distinct(['A', 'B'])",
            "df.unique(['A', 'B'])"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Option 0 is the Spark SQL method. Option 1 is an alias often found in PySpark (pandas-like). `distinct()` takes no arguments (it dedups on all columns). `unique` is not a DataFrame method."
    },
    {
        "type": "multi",
        "question": "Which of the following creates a sliding window of 1 hour duration starting every 30 minutes?",
        "options": [
            "window(col(\"time\"), \"1 hour\", \"30 minutes\")",
            "window(col(\"time\"), \"30 minutes\", \"1 hour\")",
            "sliding_window(\"time\", \"1h\", \"30m\")",
            "window(\"time\", windowDuration=\"1 hour\", slideDuration=\"30 minutes\")"
        ],
        "answer": [
            0,
            3
        ],
        "explanation": "Option 0 is the positional argument syntax (windowDuration, slideDuration). Option 3 uses keyword arguments which is also valid in Python. Option 1 has the slide larger than the window (valid, but doesn't match the prompt description of 1 hour duration). Option 2 is not a function."
    },
    {
        "type": "multi",
        "question": "What is the result of `df.coalesce(1)`?",
        "options": [
            "It reduces the number of partitions to 1 without a full shuffle.",
            "It increases the number of partitions to 1.",
            "It performs a full shuffle to repartition data.",
            "It sorts the data within one partition."
        ],
        "answer": [
            0
        ],
        "explanation": "`coalesce` is optimized to reduce partitions by merging them, avoiding a full shuffle (unlike `repartition`)."
    },
    {
        "type": "multi",
        "question": "Which method splits a string column into an array of strings?",
        "options": [
            "split()",
            "explode()",
            "tokenize()",
            "array_contains()"
        ],
        "answer": [
            0
        ],
        "explanation": "`split()` takes a string column and a delimiter and returns an array. `explode` takes an array and creates a new row for each element. `tokenize` is a Feature Transformer, not a SQL function."
    },
    {
        "type": "multi",
        "question": "Which Join strategy is triggered by `broadcast(df)`?",
        "options": [
            "Broadcast Hash Join",
            "Sort Merge Join",
            "Shuffle Hash Join",
            "Cartesian Product"
        ],
        "answer": [
            0
        ],
        "explanation": "Using the `broadcast()` hint instructs the optimizer to broadcast the smaller table to all executors, performing a Broadcast Hash Join."
    },
    {
        "type": "multi",
        "question": "Which snippet creates a `VectorAssembler` to combine columns \"a\" and \"b\" into \"features\"?",
        "options": [
            "VectorAssembler(inputCols=[\"a\", \"b\"], outputCol=\"features\")",
            "VectorAssembler().setInputCols([\"a\", \"b\"]).setOutputCol(\"features\")",
            "VectorAssembler(cols=[\"a\", \"b\"], out=\"features\")",
            "Assembler([\"a\", \"b\"], \"features\")"
        ],
        "answer": [
            0,
            1
        ],
        "explanation": "Option 0 and 1 are valid ways to initialize the transformer (constructor args or setters). The other options use incorrect parameter names or class names."
    },
    {
        "type": "multi",
        "question": "What is the purpose of `df.cache()`?",
        "options": [
            "To store the DataFrame in memory for faster subsequent access.",
            "To save the DataFrame to disk permanently.",
            "To checkpoint the DataFrame to truncate the lineage.",
            "To broadcast the DataFrame."
        ],
        "answer": [
            0
        ],
        "explanation": "`cache()` (alias for `persist` with default storage level) stores the computed data in memory (and disk if it spills) so future actions don't need to recompute the entire lineage."
    },
    {
        "type": "multi",
        "question": "Which command allows you to see the first 20 rows of a DataFrame in a tabular format?",
        "options": [
            "df.show()",
            "df.head(20)",
            "df.take(20)",
            "df.display()"
        ],
        "answer": [
            0
        ],
        "explanation": "`df.show()` defaults to 20 rows and prints a tabular ASCII format. `head` and `take` return a list of Row objects, they do not print a table. `display` is a Databricks-specific command, not core PySpark."
    }
]