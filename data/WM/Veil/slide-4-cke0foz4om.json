[
    {
        "type": "single",
        "question": "Which component of an Information Retrieval system is responsible for creating data structures like inverted indices from raw documents?",
        "options": [
            "The Query Operations Module",
            "The Document Indexer Module",
            "The Ranking/Retrieval Module",
            "The User Interface Module"
        ],
        "answer": [
            1
        ],
        "explanation": "The Indexer module processes the original raw documents to create data structures (such as inverted indices) that enable efficient retrieval. The other modules handle queries, ranking, or interaction."
    },
    {
        "type": "single",
        "question": "In the context of query types, what distinguishes a 'Phrase Query' from a standard keyword query?",
        "options": [
            "It requires terms to appear in a specific sequence",
            "It allows terms to appear anywhere in the document",
            "It automatically includes synonyms for the terms",
            "It ranks documents based on publication date"
        ],
        "answer": [
            0
        ],
        "explanation": "A phrase query demands that the terms in the query appear in the document as an exact phrase, meaning they must be adjacent and in the specified order."
    },
    {
        "type": "single",
        "question": "What is a primary limitation of the standard Boolean Information Retrieval model regarding search results?",
        "options": [
            "It cannot handle queries with more than three terms",
            "It returns documents as a set without any ranking",
            "It requires high computational power to process",
            "It often returns too many irrelevant images"
        ],
        "answer": [
            1
        ],
        "explanation": "The Boolean model operates on a binary 'match' or 'no match' basis. It retrieves a set of documents that satisfy the logic but does not calculate a relevance score to rank them."
    },
    {
        "type": "single",
        "question": "In the Vector Space Model, what is the role of the Inverse Document Frequency (IDF) component?",
        "options": [
            "To prioritize terms that appear frequently in a single document",
            "To normalize the document vector based on its total length",
            "To penalize terms that appear in many documents in the collection",
            "To increase the weight of terms that are conceptually similar"
        ],
        "answer": [
            2
        ],
        "explanation": "IDF reduces the weight of terms that occur in many documents (like common words) because they are less useful for distinguishing between relevant and non-relevant documents."
    },
    {
        "type": "single",
        "question": "Why is Cosine Similarity preferred over Euclidean Distance for measuring document relevance in the Vector Space Model?",
        "options": [
            "It is computationally faster to calculate for large matrices",
            "It normalizes for document length, focusing on orientation",
            "It accounts for the semantic meaning of the words used",
            "It works better with boolean logic queries and operators"
        ],
        "answer": [
            1
        ],
        "explanation": "Cosine similarity measures the angle between vectors, making it independent of vector magnitude (document length). Euclidean distance would penalize long documents even if they have the same topic distribution."
    },
    {
        "type": "single",
        "question": "How do Statistical Language Models typically rank documents for a given query?",
        "options": [
            "By calculating the probability that the document generated the query",
            "By counting the exact number of keyword overlaps in the text",
            "By measuring the geometric distance between term vectors",
            "By evaluating the authority of the website hosting the document"
        ],
        "answer": [
            0
        ],
        "explanation": "Statistical Language Models rank documents based on the probability P(Q|D), which is the likelihood that the query Q was generated by the probabilistic model of document D."
    },
    {
        "type": "single",
        "question": "What is the primary purpose of 'Smoothing' techniques (e.g., Laplace smoothing) in language modeling?",
        "options": [
            "To remove noise and spelling errors from the source document",
            "To assign non-zero probabilities to terms not present in a document",
            "To average the term frequencies across the entire collection",
            "To compress the size of the language model for storage"
        ],
        "answer": [
            1
        ],
        "explanation": "Smoothing handles the 'zero-probability problem' where a single missing term in a document would otherwise make the entire probability of generating the query zero. It assigns small probabilities to unseen terms."
    },
    {
        "type": "single",
        "question": "How does the Rocchio Algorithm utilize relevance feedback to improve search results?",
        "options": [
            "It modifies the query vector by moving it closer to relevant documents",
            "It deletes irrelevant documents from the database permanently",
            "It increases the PageRank score of the selected documents",
            "It re-indexes the entire collection based on user clicks"
        ],
        "answer": [
            0
        ],
        "explanation": "The Rocchio algorithm adjusts the original query vector by adding a weighted average of relevant document vectors and subtracting a weighted average of non-relevant document vectors."
    },
    {
        "type": "single",
        "question": "What is the core assumption behind 'Pseudo-Relevance Feedback'?",
        "options": [
            "The user will always provide accurate feedback on the results",
            "The top-ranked documents from the initial search are relevant",
            "The query contains all the necessary terms for retrieval",
            "The document collection is free of spam and duplicates"
        ],
        "answer": [
            1
        ],
        "explanation": "Pseudo-Relevance Feedback assumes that the top 'k' documents returned by the initial query are relevant and uses them to expand or re-weight the query without explicit user intervention."
    },
    {
        "type": "single",
        "question": "Which evaluation metric represents the fraction of retrieved documents that are actually relevant?",
        "options": [
            "Recall",
            "Precision",
            "Accuracy",
            "Specificity"
        ],
        "answer": [
            1
        ],
        "explanation": "Precision is defined as the number of relevant documents retrieved divided by the total number of documents retrieved. It measures the 'purity' of the result set."
    },
    {
        "type": "single",
        "question": "If a search engine retrieves 10 documents and 6 are relevant, but 4 relevant documents were missed, what is the Recall?",
        "options": [
            "0.40 (4/10)",
            "0.60 (6/10)",
            "0.50 (5/10)",
            "1.00 (10/10)"
        ],
        "answer": [
            1
        ],
        "explanation": "Recall is (Relevant Retrieved) / (Total Relevant). Total Relevant = 6 (retrieved) + 4 (missed) = 10. So Recall = 6/10 = 0.60. (Note: Precision would also be 0.60 here, but the calculation logic is different)."
    },
    {
        "type": "single",
        "question": "The F-score is a metric that combines Precision and Recall using which mathematical mean?",
        "options": [
            "Arithmetic Mean",
            "Geometric Mean",
            "Harmonic Mean",
            "Weighted Average"
        ],
        "answer": [
            2
        ],
        "explanation": "The F-score (specifically F1) uses the harmonic mean because it punishes extreme values more than the arithmetic mean; both precision and recall need to be high for a high F-score."
    },
    {
        "type": "single",
        "question": "Why are 'Stopwords' (e.g., 'the', 'is', 'at') typically removed during text pre-processing?",
        "options": [
            "They are difficult for the indexer to parse correctly",
            "They carry little meaning and consume significant storage",
            "They are often misspelled by users in search queries",
            "They prevent the calculation of document length"
        ],
        "answer": [
            1
        ],
        "explanation": "Stopwords are high-frequency function words that have little discriminative power for retrieval but would take up a large amount of space in the inverted index if stored."
    },
    {
        "type": "single",
        "question": "What is the main goal of the 'Stemming' process in Information Retrieval?",
        "options": [
            "To reduce words to their base or root form",
            "To correct grammatical errors in the text",
            "To translate words into a standard language",
            "To identify proper nouns and entities"
        ],
        "answer": [
            0
        ],
        "explanation": "Stemming conflates variant forms of a word (e.g., 'fishing', 'fished', 'fisher') into a common root (e.g., 'fish') to increase recall by matching related terms."
    },
    {
        "type": "single",
        "question": "In the context of an Inverted Index, what does a 'Postings List' typically contain?",
        "options": [
            "A dictionary of all unique terms in the collection",
            "A list of document IDs that contain a specific term",
            "A matrix of term frequencies for every document",
            "A set of hyperlinks pointing to a specific page"
        ],
        "answer": [
            1
        ],
        "explanation": "For each term in the vocabulary, the inverted index maintains a postings list, which is a list of unique identifiers (DocIDs) for the documents where that term appears."
    },
    {
        "type": "single",
        "question": "Which of the following best describes the 'Variable-Byte Coding' compression method?",
        "options": [
            "It uses a fixed 32-bit integer for every document ID",
            "It uses 7 bits for data and 1 bit as a continuation flag",
            "It uses a unary code for the quotient and binary for the remainder",
            "It uses a dictionary to replace common phrases with tokens"
        ],
        "answer": [
            1
        ],
        "explanation": "Variable-byte coding uses a sequence of bytes to store a number. The first 7 bits of each byte are data, and the 8th bit is a flag indicating whether the number continues in the next byte."
    },
    {
        "type": "single",
        "question": "What is the primary advantage of storing 'Gaps' (d-gaps) instead of raw Document IDs in an inverted index?",
        "options": [
            "It allows the list to be sorted more quickly",
            "It results in smaller numbers that are easier to compress",
            "It prevents the reconstruction of the original text",
            "It enables faster random access to the document list"
        ],
        "answer": [
            1
        ],
        "explanation": "Since DocIDs are sorted, the difference between adjacent IDs (the gap) is usually much smaller than the ID itself. Smaller integers require fewer bits to encode, improving compression."
    },
    {
        "type": "single",
        "question": "Latent Semantic Indexing (LSI) uses Singular Value Decomposition (SVD) to address which problem?",
        "options": [
            "The high computational cost of boolean queries",
            "The mismatch of vocabulary (synonymy/polysemy)",
            "The presence of spam in web documents",
            "The duplicate content on the web"
        ],
        "answer": [
            1
        ],
        "explanation": "LSI maps terms and documents to a lower-dimensional 'concept space'. This helps match queries to documents that use different words for the same concept (synonymy) or disambiguate terms (polysemy)."
    },
    {
        "type": "single",
        "question": "What is a significant disadvantage of using Latent Semantic Indexing (LSI) on large collections?",
        "options": [
            "The SVD computation is computationally very expensive",
            "It cannot handle documents written in English",
            "It increases the size of the index by a factor of 10",
            "It requires manual tagging of all documents"
        ],
        "answer": [
            0
        ],
        "explanation": "Computing the SVD of a large matrix is computationally intensive (cubic complexity in naive implementations), making standard LSI difficult to scale to massive web-scale collections."
    },
    {
        "type": "single",
        "question": "Why is 'Anchor Text' considered highly valuable for Web Search indexing?",
        "options": [
            "It is the only part of a page that is guaranteed to be unique",
            "It often describes the target page more accurately than the page itself",
            "It contains hidden metadata that is useful for ranking",
            "It is always written by the author of the target page"
        ],
        "answer": [
            1
        ],
        "explanation": "Anchor text represents how other people describe a page. It is often a concise and accurate summary of the target page's content, sometimes containing terms not even present on the target page."
    },
    {
        "type": "single",
        "question": "What is the defining characteristic of a 'Meta-Search' engine?",
        "options": [
            "It maintains the largest index of the deep web",
            "It aggregates and ranks results from multiple other search engines",
            "It focuses exclusively on searching metadata tags",
            "It uses a peer-to-peer network to store documents"
        ],
        "answer": [
            1
        ],
        "explanation": "A meta-search engine does not have its own crawler or index. Instead, it sends the user's query to several other search engines and combines their results into a single ranked list."
    },
    {
        "type": "single",
        "question": "In Meta-Search fusion, how does the 'CombMNZ' (Combine Multiply Non-Zero) method work?",
        "options": [
            "It averages the scores and subtracts the lowest one",
            "It sums the scores and multiplies by the count of returning engines",
            "It selects the single highest score found across all lists",
            "It multiplies all scores together to penalize zeros"
        ],
        "answer": [
            1
        ],
        "explanation": "CombMNZ sums the similarity scores of a document from all engines and multiplies this sum by the number of engines that returned that document (non-zero score), boosting widely agreed-upon results."
    },
    {
        "type": "single",
        "question": "The 'Borda Ranking' method in meta-search combines results based on which factor?",
        "options": [
            "The similarity score provided by the engines",
            "The points assigned based on the rank position in each list",
            "The number of times the document has been clicked",
            "The number of shared keywords between documents"
        ],
        "answer": [
            1
        ],
        "explanation": "Borda Ranking is a positional voting method. A document gets points based on its position in each list (e.g., top rank gets n points), and the total points determine the final rank."
    },
    {
        "type": "single",
        "question": "Which meta-search ranking method uses a pairwise voting scheme to determine the 'winner'?",
        "options": [
            "Condorcet Ranking",
            "Borda Count",
            "CombSUM",
            "Reciprocal Rank"
        ],
        "answer": [
            0
        ],
        "explanation": "Condorcet ranking compares every document against every other document. A document wins if it is ranked higher than the opponent in a majority of the search engines' lists."
    },
    {
        "type": "single",
        "question": "Which of the following is a technique used in 'Content Spamming'?",
        "options": [
            "Creating a link farm to boost authority",
            "Stuffing the page with popular but irrelevant keywords",
            "Hijacking a browser's home page setting",
            "Intercepting query traffic from other users"
        ],
        "answer": [
            1
        ],
        "explanation": "Content spamming involves manipulating the text of the page, such as repeating keywords or adding popular unrelated terms (stuffing), to trick the engine into ranking it for those queries."
    },
    {
        "type": "single",
        "question": "What is the main function of a 'Honey Pot' in the context of Link Spamming?",
        "options": [
            "To capture and analyze the IP addresses of spammers",
            "To attract legitimate links which are then funnelled to spam pages",
            "To store a backup of the spam content in case of deletion",
            "To block search engine crawlers from accessing the site"
        ],
        "answer": [
            1
        ],
        "explanation": "A honey pot is a page containing some useful content designed to attract links from legitimate sites. It then contains hidden or inconspicuous links to the spammer's target pages to transfer authority."
    },
    {
        "type": "single",
        "question": "The technique of serving one version of a page to a crawler and a different version to a human user is called:",
        "options": [
            "Mirroring",
            "Cloaking",
            "Redirecting",
            "Caching"
        ],
        "answer": [
            1
        ],
        "explanation": "Cloaking involves detecting the requestor (e.g., by IP or User-Agent). If it's a crawler, an optimized/spammy page is sent; if it's a user, a normal page is sent."
    },
    {
        "type": "single",
        "question": "Which feature might a search engine analyze to detect 'Content Spam' based on statistical properties?",
        "options": [
            "The average word length of the text on the page",
            "The number of images relative to the text size",
            "The version of HTML used to build the page",
            "The physical location of the web server"
        ],
        "answer": [
            0
        ],
        "explanation": "Synthetic or stuffed content often deviates from the statistical norms of natural language, such as the average word length (approx 5 letters for English). Search engines analyze this to flag spam."
    },
    {
        "type": "single",
        "question": "What is 'Link Exchange' in the context of Web Spam?",
        "options": [
            "A protocol for transferring hyperlinks between servers",
            "A group of sites agreeing to link to each other to boost rank",
            "A method for repairing broken links automatically",
            "A system for tracking user clicks on advertisements"
        ],
        "answer": [
            1
        ],
        "explanation": "Link exchange is a form of collusion where a group of webmasters link to each other's sites specifically to artificially inflate their link popularity and Hub/Authority scores."
    },
    {
        "type": "single",
        "question": "In 'Out-link Spamming', what strategy does a spammer typically employ?",
        "options": [
            "Linking to their own pages to create a closed loop",
            "Linking to authoritative sites to boost their own Hub score",
            "Removing all outgoing links to preserve PageRank",
            "Randomly linking to non-existent pages"
        ],
        "answer": [
            1
        ],
        "explanation": "By linking to known authoritative sites (like major news outlets or universities), a spammer attempts to increase their page's 'Hub' score, hoping this will in turn boost the 'Authority' score of other linked pages."
    },
    {
        "type": "single",
        "question": "What is 'Reciprocal Ranking' commonly used for in the context of Meta-Search?",
        "options": [
            "Fusing ranked lists by summing the inverse of rankings",
            "Determining the reciprocal links between websites",
            "Calculating the inverse document frequency of terms",
            "Ranking documents based on the date of publication"
        ],
        "answer": [
            0
        ],
        "explanation": "Reciprocal Ranking fuses results by assigning a score of 1/r (where r is the rank) to a document for each list it appears in, then summing these scores."
    },
    {
        "type": "single",
        "question": "In the 'Elias Gamma Coding' compression scheme, how is a number x typically represented?",
        "options": [
            "As a fixed-length 8-bit binary code",
            "As a unary code for length followed by a binary suffix",
            "As a sequence of 7-bit chunks with continuation bits",
            "As a direct mapping to a character in a dictionary"
        ],
        "answer": [
            1
        ],
        "explanation": "Elias Gamma coding represents an integer x by encoding its length (order of magnitude) in unary, followed by the binary representation of x (excluding the leading 1)."
    },
    {
        "type": "single",
        "question": "Which of the following is a method used to 'Hide' spam content from visible view?",
        "options": [
            "Using a font color that is identical to the background color",
            "Using a very large font size for keywords",
            "Placing keywords in the meta description tag",
            "Highlighting keywords with bold or italic tags"
        ],
        "answer": [
            0
        ],
        "explanation": "A common hiding technique is to make text the same color as the background (e.g., white on white), so it is invisible to the user but still indexed by the crawler."
    },
    {
        "type": "single",
        "question": "What is the primary role of the 'Query Operations' module in an IR system?",
        "options": [
            "To store the crawled documents in a database",
            "To transform a user's natural language into a system query",
            "To calculate the final ranking of the documents",
            "To manage the user's login and search history"
        ],
        "answer": [
            1
        ],
        "explanation": "The Query Operations module sits between the user and the system, parsing the user's request (e.g., natural language) and converting it into a formal query format the system can execute."
    },
    {
        "type": "single",
        "question": "Which of the following statements is true regarding 'Proximity Queries'?",
        "options": [
            "They require terms to appear within a specific distance of each other",
            "They only search for terms that are synonyms of the query",
            "They rank documents based on the geographical proximity of the server",
            "They are strictly faster to execute than standard boolean queries"
        ],
        "answer": [
            0
        ],
        "explanation": "Proximity queries place a constraint on the distance (number of intervening words) between query terms, allowing for more precise retrieval than simple AND logic."
    },
    {
        "type": "single",
        "question": "In the context of text pre-processing, what does 'Case Folding' refer to?",
        "options": [
            "Converting all characters to a standard case (usually lowercase)",
            "Folding the text to fit within a specific column width",
            "Removing all punctuation marks from the document",
            "Grouping documents into folders based on case sensitivity"
        ],
        "answer": [
            0
        ],
        "explanation": "Case folding converts all text to a uniform case (usually lowercase) so that 'Computer', 'COMPUTER', and 'computer' are treated as the same token."
    },
    {
        "type": "single",
        "question": "What is the 'Vocabulary Search' step in the retrieval process?",
        "options": [
            "Searching the inverted index's dictionary for query terms",
            "Searching the web for new words to add to the system",
            "Searching for synonyms in an external thesaurus",
            "Searching for definitions of unknown words"
        ],
        "answer": [
            0
        ],
        "explanation": "Vocabulary Search is the first step of retrieval where the system looks up the query terms in the dictionary (vocabulary) of the inverted index to find their corresponding postings lists."
    },
    {
        "type": "single",
        "question": "Which index compression technique is specifically noted for being 'byte-aligned'?",
        "options": [
            "Variable-Byte Coding",
            "Elias Gamma Coding",
            "Elias Delta Coding",
            "Unary Coding"
        ],
        "answer": [
            0
        ],
        "explanation": "Variable-Byte coding uses integral bytes (8 bits), making it byte-aligned and faster to decode on standard hardware compared to bit-level schemes like Elias Gamma."
    },
    {
        "type": "single",
        "question": "What does the 'Term Frequency' (TF) assume about the relationship between words and documents?",
        "options": [
            "If a term appears often in a document, it is important to that document",
            "If a term appears often in the collection, it is a useful discriminator",
            "If a term is long, it carries more semantic meaning",
            "If a term is a noun, it is more important than a verb"
        ],
        "answer": [
            0
        ],
        "explanation": "TF assumes that the more frequently a term appears within a specific document, the more relevant that document is to the concept represented by the term."
    },
    {
        "type": "single",
        "question": "In the Vector Space Model, a document is conceptually represented as:",
        "options": [
            "A point or vector in a high-dimensional space",
            "A hierarchical tree of topics and subtopics",
            "A linked list of keywords and their definitions",
            "A binary string indicating term presence"
        ],
        "answer": [
            0
        ],
        "explanation": "The Vector Space Model represents documents (and queries) as vectors in a multi-dimensional space, where each dimension corresponds to a term in the vocabulary."
    }
]