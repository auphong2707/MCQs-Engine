[
    {
        "type": "single",
        "question": "A churn model is tested on a dataset with 950 Non-Churners and 50 Churners. It predicts 'Non-Churn' for every single customer. What are the Accuracy and Recall of this model?",
        "options": [
            "Accuracy = 95%; Recall = 0%",
            "Accuracy = 5%; Recall = 100%",
            "Accuracy = 95%; Recall = 100%",
            "Accuracy = 50%; Recall = 50%"
        ],
        "answer": [
            0
        ],
        "explanation": "Accuracy is (TP+TN)/Total. Here, TN=950, TP=0, Total=1000, so Accuracy = 950/1000 = 95%. Recall is TP/(TP+FN). Since the model predicted 0 positives, TP=0, making Recall 0%. This illustrates why accuracy is misleading for imbalanced classes."
    },
    {
        "type": "single",
        "question": "Calculate the F1-score for a classifier with the following metrics: Precision = 0.60, Recall = 0.40.",
        "options": [
            "0.50",
            "0.48",
            "0.24",
            "0.52"
        ],
        "answer": [
            1
        ],
        "explanation": "The F1-score is the harmonic mean of Precision and Recall: F1 = 2 * (Precision * Recall) / (Precision + Recall). Calculation: 2 * (0.24) / (1.0) = 0.48."
    },
    {
        "type": "single",
        "question": "In a medical diagnosis scenario where a False Negative (missing a sick patient) costs $10,000 and a False Positive (unnecessary test) costs $100, which threshold adjustment is most logical?",
        "options": [
            "Increase the threshold to maximize Precision.",
            "Decrease the threshold to maximize Recall.",
            "Set the threshold to 0.5 regardless of cost.",
            "Increase the threshold to maximize Specificity."
        ],
        "answer": [
            1
        ],
        "explanation": "Since the cost of a False Negative is significantly higher ($10,000 vs $100), the priority is to catch as many positive cases as possible. Lowering the threshold increases Recall (reducing False Negatives) at the expense of Precision (increasing False Positives), which is the economically correct trade-off here."
    },
    {
        "type": "single",
        "question": "Given the confusion matrix: TP=40, FP=60, FN=10, TN=890. Calculate the False Positive Rate (FPR).",
        "options": [
            "0.063",
            "0.043",
            "0.110",
            "0.011"
        ],
        "answer": [
            0
        ],
        "explanation": "FPR = FP / (FP + TN). Here, FP=60 and TN=890. Total Negatives = 950. FPR = 60 / 950 ≈ 0.063 (6.3%)."
    },
    {
        "type": "single",
        "question": "You are analyzing a KPI Driver Tree. If 'Total Revenue' is the parent node, and its children are 'Traffic Volume' and 'Conversion Rate', what is the mathematical relationship between these child nodes?",
        "options": [
            "Additive (Traffic + Conversion Rate)",
            "Subtractive (Traffic - Conversion Rate)",
            "Multiplicative (Traffic × Conversion Rate)",
            "Divisive (Traffic / Conversion Rate)"
        ],
        "answer": [
            2
        ],
        "explanation": "In a KPI tree, rate metrics and volume metrics typically have a multiplicative relationship to produce the outcome. Revenue is a function of the volume of users multiplied by the rate at which they convert (and usually AOV)."
    },
    {
        "type": "single",
        "question": "Which of the following is the primary reason for normalizing (scaling) data before running K-means clustering?",
        "options": [
            "To convert categorical variables into numeric values.",
            "To ensure that features with larger ranges (e.g., Salary) do not dominate the distance calculation over features with smaller ranges (e.g., Age).",
            "To reduce the number of dimensions in the dataset.",
            "To remove outliers from the dataset."
        ],
        "answer": [
            1
        ],
        "explanation": "K-means relies on Euclidean distance. If one feature has a range of 0-100,000 and another 0-100, the distance will be almost entirely determined by the larger feature. Scaling (e.g., Z-score) ensures all features contribute equally."
    },
    {
        "type": "single",
        "question": "A classifier has the following ROC coordinates (FPR, TPR) for different thresholds: A(0.1, 0.4), B(0.2, 0.7), C(0.4, 0.8). Which point has the highest Youden’s J Index?",
        "options": [
            "Point A",
            "Point B",
            "Point C",
            "All points have equal J Index."
        ],
        "answer": [
            1
        ],
        "explanation": "Youden’s J = TPR - FPR. Point A: 0.4 - 0.1 = 0.3. Point B: 0.7 - 0.2 = 0.5. Point C: 0.8 - 0.4 = 0.4. Point B has the highest index (0.5)."
    },
    {
        "type": "single",
        "question": "In Principal Component Analysis (PCA), if the first three eigenvalues are λ1=5.0, λ2=3.0, λ3=2.0, what is the Total Variance of the dataset (assuming these are the only components)?",
        "options": [
            "5.0",
            "3.33",
            "10.0",
            "1.0"
        ],
        "answer": [
            2
        ],
        "explanation": "The Total Variance is the sum of all eigenvalues. 5.0 + 3.0 + 2.0 = 10.0."
    },
    {
        "type": "single",
        "question": "Using the eigenvalues λ1=5.0, λ2=3.0, λ3=2.0, what is the Explained Variance Ratio of the first component (PC1)?",
        "options": [
            "50%",
            "30%",
            "20%",
            "100%"
        ],
        "answer": [
            0
        ],
        "explanation": "Explained Variance Ratio = λ1 / Total Variance. From the previous calculation, Total Variance = 10.0. Ratio = 5.0 / 10.0 = 0.5 or 50%."
    },
    {
        "type": "single",
        "question": "Which evaluation metric is most appropriate for a Fraud Detection model where valid transactions (Negative) vastly outnumber fraud cases (Positive), and we care primarily about the 'usefulness' of our fraud alerts?",
        "options": [
            "Accuracy",
            "Precision-Recall (PR) Curve",
            "ROC Curve",
            "Specificity"
        ],
        "answer": [
            1
        ],
        "explanation": "When the negative class is huge (imbalanced), ROC curves can look overly optimistic because the FPR (FP/TN) stays small due to the large TN. The PR curve focuses only on the Positive class (Precision and Recall), giving a more honest view of performance."
    },
    {
        "type": "single",
        "question": "In a 1D K-means iteration, Cluster 1 has points {2, 4, 9}. What is the new location of the centroid for Cluster 1 during the Update step?",
        "options": [
            "4",
            "5",
            "7.5",
            "9"
        ],
        "answer": [
            1
        ],
        "explanation": "The update step moves the centroid to the mean (average) of the points assigned to the cluster. Mean = (2 + 4 + 9) / 3 = 15 / 3 = 5."
    },
    {
        "type": "single",
        "question": "Identify the 'Leading Indicator' in the context of an E-commerce sales dashboard.",
        "options": [
            "Monthly Total Revenue",
            "Customer Churn Rate last quarter",
            "Cart Abandonment Rate (real-time)",
            "Net Profit Margin"
        ],
        "answer": [
            2
        ],
        "explanation": "Revenue, Churn, and Profit are Lagging Indicators (outputs that have already happened). Cart Abandonment Rate is a Leading Indicator (input/driver) because high abandonment predicts lower future sales and can be acted upon immediately."
    },
    {
        "type": "single",
        "question": "If you observe a Silhouette Score of -0.8 for a data point, what does this indicate?",
        "options": [
            "The point is very close to its cluster center.",
            "The point is likely assigned to the wrong cluster.",
            "The point is an outlier that belongs to no cluster.",
            "The clustering is perfectly separated."
        ],
        "answer": [
            1
        ],
        "explanation": "The Silhouette Score ranges from -1 to +1. A score near +1 means the point is well-clustered. A negative score indicates the point is closer to a neighboring cluster than to its assigned cluster, implying a misclassification."
    },
    {
        "type": "single",
        "question": "In Time Series decomposition, what does the 'Seasonality' component represent?",
        "options": [
            "The long-term progression of the series (increasing or decreasing).",
            "Random, irregular noise in the data.",
            "Regular, repeating patterns at fixed intervals (e.g., weekly, yearly).",
            "The effect of external variables like holidays."
        ],
        "answer": [
            2
        ],
        "explanation": "Seasonality captures repeating patterns with a fixed period, such as higher sales every December or lower traffic every weekend. Trends capture long-term direction, and Remainders capture noise."
    },
    {
        "type": "single",
        "question": "A model has a Specificity of 90% and a Sensitivity (Recall) of 70%. What is its Balanced Accuracy?",
        "options": [
            "80%",
            "63%",
            "20%",
            "90%"
        ],
        "answer": [
            0
        ],
        "explanation": "Balanced Accuracy is the arithmetic mean of Sensitivity and Specificity. Calculation: (0.90 + 0.70) / 2 = 1.60 / 2 = 0.80 or 80%."
    },
    {
        "type": "single",
        "question": "Why is 'Random Sampling' (k-fold cross-validation) generally inappropriate for Time Series data?",
        "options": [
            "It requires too much computational power.",
            "It violates the temporal dependence of the data (using the future to predict the past).",
            "It cannot handle missing values.",
            "It only works for regression, not classification."
        ],
        "answer": [
            1
        ],
        "explanation": "Time series data is ordered. Standard k-fold CV randomly shuffles data, meaning a model might be trained on data from 2025 to predict data from 2024. This 'leakage' from the future invalidates the test. Time Series Cross-Validation (Rolling Origin) preserves the chronological order."
    },
    {
        "type": "single",
        "question": "In an 'Elbow Plot' (Inertia vs. k), how do you select the optimal number of clusters?",
        "options": [
            "Pick the k with the lowest possible Inertia.",
            "Pick the k with the highest possible Inertia.",
            "Pick the k at the point where the rate of decrease in Inertia slows down sharply.",
            "Pick the k that equals the number of variables in the dataset."
        ],
        "answer": [
            2
        ],
        "explanation": "The goal is to balance compactness (low Inertia) with complexity (low k). The 'Elbow' marks the point of diminishing returns, where adding more clusters yields only marginal gains in compactness."
    },
    {
        "type": "single",
        "question": "A Funnel Analysis shows the following user counts: Step 1 (1000) -> Step 2 (500) -> Step 3 (400) -> Step 4 (100). What is the conversion rate from Step 3 to Step 4?",
        "options": [
            "10%",
            "25%",
            "40%",
            "20%"
        ],
        "answer": [
            1
        ],
        "explanation": "Conversion Rate = (Users at Next Step / Users at Current Step). CR = 100 / 400 = 0.25 or 25%."
    },
    {
        "type": "single",
        "question": "Which statement correctly describes the difference between Leading and Lagging indicators?",
        "options": [
            "Leading indicators are easier to measure than lagging indicators.",
            "Lagging indicators predict the future, while leading indicators explain the past.",
            "Lagging indicators measure output outcomes, while leading indicators measure input activities that influence those outcomes.",
            "There is no difference; they are synonyms."
        ],
        "answer": [
            2
        ],
        "explanation": "Lagging indicators (e.g., Revenue) confirm what has already happened. Leading indicators (e.g., Sales Calls Made) are actionable inputs that drive the future value of the lagging indicators."
    },
    {
        "type": "single",
        "question": "Calculate the Recall given: True Positives = 80, False Negatives = 20.",
        "options": [
            "80%",
            "20%",
            "75%",
            "60%"
        ],
        "answer": [
            0
        ],
        "explanation": "Recall = TP / (TP + FN). Recall = 80 / (80 + 20) = 80 / 100 = 80%."
    },
    {
        "type": "single",
        "question": "Which component of an ARIMA model addresses the 'Trend' in a non-stationary time series?",
        "options": [
            "AR (Auto-Regressive)",
            "I (Integrated / Differencing)",
            "MA (Moving Average)",
            "X (Exogenous variables)"
        ],
        "answer": [
            1
        ],
        "explanation": "The 'I' in ARIMA stands for Integrated, which refers to the differencing step ($d$). Differencing is used to remove trends and stabilize the mean, making the series stationary."
    },
    {
        "type": "single",
        "question": "In K-means clustering, what happens during the 'Assignment Step'?",
        "options": [
            "Centroids are moved to the center of their clusters.",
            "The number of clusters (k) is automatically determined.",
            "Each data point is assigned to the nearest centroid based on Euclidean distance.",
            "Outliers are removed from the dataset."
        ],
        "answer": [
            2
        ],
        "explanation": "The algorithm consists of two repeated steps. In the Assignment step, points are grouped by proximity to the current centroids. In the Update step, the centroids are moved."
    },
    {
        "type": "single",
        "question": "A business calculates a 'Total Expected Cost' for a model. If TP and TN have zero cost, FP costs $5, and FN costs $50, what is the total cost for a model with 10 FPs and 2 FNs?",
        "options": [
            "$150",
            "$550",
            "$60",
            "$100"
        ],
        "answer": [
            0
        ],
        "explanation": "Total Cost = (FP count * Cost FP) + (FN count * Cost FN). Cost = (10 * 5) + (2 * 50) = 50 + 100 = $150."
    },
    {
        "type": "single",
        "question": "What is the primary definition of a 'Stationary' time series?",
        "options": [
            "A series that constantly increases over time.",
            "A series whose statistical properties (mean, variance) do not change over time.",
            "A series with zero missing values.",
            "A series that perfectly repeats itself every year."
        ],
        "answer": [
            1
        ],
        "explanation": "Stationarity is a core assumption for many forecasting models (like ARIMA). It implies that the data has a constant mean, constant variance, and autocovariance that depends only on the lag, not on the time $t$."
    },
    {
        "type": "single",
        "question": "In a confusion matrix, which metric represents the 'Precision' (PPV)?",
        "options": [
            "TP / (TP + FP)",
            "TP / (TP + FN)",
            "TN / (TN + FP)",
            "FP / (FP + TN)"
        ],
        "answer": [
            0
        ],
        "explanation": "Precision measures the accuracy of positive predictions: of all the times the model said 'Yes', how many were actually 'Yes'? The denominator is Predicted Positives (TP + FP)."
    },
    {
        "type": "single",
        "question": "When interpreting a KPI driver tree, if 'Average Order Value' drops while 'Order Frequency' rises, and 'Revenue' remains flat, what is the most likely diagnosis?",
        "options": [
            "The company is losing customers.",
            "The company is successfully upselling expensive products.",
            "Customers are buying cheaper items more often, offsetting the volume gain with lower value per unit.",
            "Marketing spend has increased."
        ],
        "answer": [
            2
        ],
        "explanation": "Since Revenue = Frequency * AOV, if Revenue is flat and Frequency is up, AOV must be down. This indicates a shift in behavior where users transact more frequently but spend less per transaction (e.g., shifting from premium to budget items)."
    },
    {
        "type": "single",
        "question": "Which of the following is NOT a requirement for K-means clustering?",
        "options": [
            "Specifying the number of clusters (k) beforehand.",
            "Using a distance metric (usually Euclidean).",
            "The dataset must be labeled (Supervised Learning).",
            "Scaling/Standardizing variables."
        ],
        "answer": [
            2
        ],
        "explanation": "K-means is an Unsupervised Learning algorithm. It does not require (and does not use) target labels. It finds structure in the data based solely on feature similarity."
    },
    {
        "type": "single",
        "question": "Calculate the specificity (TNR) if TN=90 and FP=10.",
        "options": [
            "90%",
            "10%",
            "80%",
            "100%"
        ],
        "answer": [
            0
        ],
        "explanation": "Specificity = TN / (TN + FP). Specificity = 90 / (90 + 10) = 90 / 100 = 90%."
    },
    {
        "type": "single",
        "question": "In PCA, what does the first Principal Component (PC1) represent?",
        "options": [
            "The direction of minimum variance in the data.",
            "The feature with the highest correlation to the target.",
            "The direction in the data that captures the maximum possible variance.",
            "The mean of all features."
        ],
        "answer": [
            2
        ],
        "explanation": "PCA seeks to project data onto orthogonal axes. PC1 is the axis that maximizes the variance of the projected data, effectively capturing the most 'information' or 'spread'."
    },
    {
        "type": "single",
        "question": "A user moves from Step 2 (500 users) to Step 3 (400 users). If a UI fix improves this conversion rate to 90%, how many users would reach Step 3 (assuming Step 2 stays constant)?",
        "options": [
            "400",
            "450",
            "500",
            "360"
        ],
        "answer": [
            1
        ],
        "explanation": "New Users at Step 3 = Users at Step 2 * New Rate. 500 * 0.90 = 450 users."
    },
    {
        "type": "single",
        "question": "Which metric focuses specifically on the 'cost of false alarms'?",
        "options": [
            "Recall",
            "Precision",
            "Recall and Specificity",
            "F1-score"
        ],
        "answer": [
            1
        ],
        "explanation": "Precision (TP / (TP+FP)) is the metric that is penalized by False Positives (false alarms). Low precision means many alerts are false, incurring the 'cost of false alarms'."
    },
    {
        "type": "single",
        "question": "Why might a business prefer a 'Soft Clustering' approach (like GMM) over K-means?",
        "options": [
            "It is faster to compute.",
            "It allows a data point to belong partially to multiple clusters (probabilities) rather than a strict assignment.",
            "It works without defining the number of clusters.",
            "It does not require feature scaling."
        ],
        "answer": [
            1
        ],
        "explanation": "K-means is 'hard' clustering (point P is in Cluster A, period). Gaussian Mixture Models (GMM) provide 'soft' clustering, outputting the probability that point P belongs to Cluster A vs B, which is useful for ambiguous cases."
    },
    {
        "type": "single",
        "question": "In Time Series analysis, what is the 'Naive Forecast' for time t+1?",
        "options": [
            "The average of all previous values.",
            "The value at time t (the most recent observation).",
            "A linear regression projection.",
            "Zero."
        ],
        "answer": [
            1
        ],
        "explanation": "The Naive method simply assumes that the next value will be equal to the last observed value ($y_{t+1} = y_t$). It is a standard benchmark to see if complex models are actually adding value."
    },
    {
        "type": "single",
        "question": "What happens to the Recall of a model if you lower the decision threshold from 0.5 to 0.3?",
        "options": [
            "Recall will likely increase.",
            "Recall will likely decrease.",
            "Recall will stay exactly the same.",
            "Recall will become 0."
        ],
        "answer": [
            0
        ],
        "explanation": "Lowering the threshold makes it 'easier' to predict Positive. This captures more actual positives (increasing TP) but also captures more negatives (increasing FP). Higher TP with constant Total Positives results in higher Recall."
    },
    {
        "type": "single",
        "question": "In a KPI tree, if Revenue = Traffic * Conversion * AOV, and Traffic increases by 10% while others stay flat, what happens to Revenue?",
        "options": [
            "Revenue increases by 10%.",
            "Revenue increases by 30%.",
            "Revenue stays flat.",
            "Revenue decreases."
        ],
        "answer": [
            0
        ],
        "explanation": "Due to the multiplicative nature, a 10% lift in one factor (1.10) scales the entire equation by 1.10, assuming other factors remain constant (1.0 * 1.0)."
    },
    {
        "type": "single",
        "question": "You calculate a total Euclidean distance of 0 between two data points. What does this imply?",
        "options": [
            "The points are identical.",
            "The points are very far apart.",
            "The calculation is an error.",
            "The points are on opposite sides of the plot."
        ],
        "answer": [
            0
        ],
        "explanation": "Euclidean distance is the square root of the sum of squared differences. The only way for the distance to be exactly 0 is if the difference for every single feature is 0, meaning the points occupy the exact same coordinate."
    },
    {
        "type": "single",
        "question": "Which concept describes the problem where adding more dimensions (features) to a dataset eventually degrades clustering performance?",
        "options": [
            "The Elbow Rule",
            "The Curse of Dimensionality",
            "Overfitting",
            "Underfitting"
        ],
        "answer": [
            1
        ],
        "explanation": "The Curse of Dimensionality refers to phenomena where data becomes sparse in high-dimensional space, making distance metrics (like Euclidean) less meaningful and clustering less effective."
    },
    {
        "type": "single",
        "question": "If a Time Series model predicts sales of 100, 110, 120 for the next 3 days, but actual sales are 90, 115, 130, what is the Mean Absolute Error (MAE)?",
        "options": [
            "10",
            "8.33",
            "25",
            "-5"
        ],
        "answer": [
            1
        ],
        "explanation": "Errors are |100-90|=10, |110-115|=5, |120-130|=10. Sum of absolute errors = 10+5+10 = 25. Mean = 25 / 3 ≈ 8.33."
    },
    {
        "type": "single",
        "question": "Which type of variable is necessary to account for 'Holidays' in a Dynamic Regression time series model?",
        "options": [
            "Continuous variable",
            "Dummy (Binary) variable",
            "Lagged variable",
            "Random variable"
        ],
        "answer": [
            1
        ],
        "explanation": "Holidays are categorical events (It is Christmas or it is not). They are encoded as Dummy variables (0 or 1) to allow the regression model to assign a specific coefficient (impact) to those days."
    },
    {
        "type": "single",
        "question": "In the context of Prescriptive Analytics for funnel optimization, what is the 'Leaky Bucket' theory?",
        "options": [
            "You should pour more water (marketing spend) into the bucket to keep it full.",
            "You should fix the holes (conversion drops) before adding more water (traffic), otherwise resources are wasted.",
            "Buckets with holes are useless and should be discarded.",
            "The bucket represents the customer lifetime value."
        ],
        "answer": [
            1
        ],
        "explanation": "The Leaky Bucket analogy argues that increasing traffic (water) is inefficient if the funnel (bucket) has low conversion rates (holes). The prescriptive action is to fix the product experience first."
    }
]